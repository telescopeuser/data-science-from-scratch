{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# KudosData.com\n",
    "###############\n",
    "#\n",
    "import matplotlib\n",
    "# Force matplotlib to not use any Xwindows backend.\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math, random, re\n",
    "from collections import defaultdict, Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def plot_resumes(plt):\n",
    "    data = [ (\"big data\", 100, 15), (\"Hadoop\", 95, 25), (\"Python\", 75, 50),\n",
    "         (\"R\", 50, 40), (\"machine learning\", 80, 20), (\"statistics\", 20, 60),\n",
    "         (\"data science\", 60, 70), (\"analytics\", 90, 3),\n",
    "         (\"team player\", 85, 85), (\"dynamic\", 2, 90), (\"synergies\", 70, 0),\n",
    "         (\"actionable insights\", 40, 30), (\"think out of the box\", 45, 10),\n",
    "         (\"self-starter\", 30, 50), (\"customer focus\", 65, 15),\n",
    "         (\"thought leadership\", 35, 35)]\n",
    "\n",
    "    def text_size(total):\n",
    "        \"\"\"equals 8 if total is 0, 28 if total is 200\"\"\"\n",
    "        return 8 + total / 200 * 20\n",
    "\n",
    "    for word, job_popularity, resume_popularity in data:\n",
    "        plt.text(job_popularity, resume_popularity, word,\n",
    "                 ha='center', va='center',\n",
    "                 size=text_size(job_popularity + resume_popularity))\n",
    "    plt.xlabel(\"Popularity on Job Postings\")\n",
    "    plt.ylabel(\"Popularity on Resumes\")\n",
    "    plt.axis([0, 100, 0, 100])\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "# n-gram models\n",
    "#\n",
    "\n",
    "def fix_unicode(text):\n",
    "    return text.replace(u\"\\u2019\", \"'\")\n",
    "\n",
    "def get_document():\n",
    "\n",
    "    url = \"http://radar.oreilly.com/2010/06/what-is-data-science.html\"\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "    content = soup.find(\"div\", \"article-body\")        # find article-body div\n",
    "    regex = r\"[\\w']+|[\\.]\"                            # matches a word or a period\n",
    "\n",
    "    document = []\n",
    "\n",
    "\n",
    "    for paragraph in content(\"p\"):\n",
    "        words = re.findall(regex, fix_unicode(paragraph.text))\n",
    "        document.extend(words)\n",
    "\n",
    "    return document\n",
    "\n",
    "def generate_using_bigrams(transitions):\n",
    "    current = \".\"   # this means the next word will start a sentence\n",
    "    result = []\n",
    "    while True:\n",
    "        next_word_candidates = transitions[current]    # bigrams (current, _)\n",
    "        current = random.choice(next_word_candidates)  # choose one at random\n",
    "        result.append(current)                         # append it to results\n",
    "        if current == \".\": return \" \".join(result)     # if \".\" we're done\n",
    "\n",
    "def generate_using_trigrams(starts, trigram_transitions):\n",
    "    current = random.choice(starts)   # choose a random starting word\n",
    "    prev = \".\"                        # and precede it with a '.'\n",
    "    result = [current]\n",
    "    while True:\n",
    "        next_word_candidates = trigram_transitions[(prev, current)]\n",
    "        next = random.choice(next_word_candidates)\n",
    "\n",
    "        prev, current = current, next\n",
    "        result.append(current)\n",
    "\n",
    "        if current == \".\":\n",
    "            return \" \".join(result)\n",
    "\n",
    "def is_terminal(token):\n",
    "    return token[0] != \"_\"\n",
    "\n",
    "def expand(grammar, tokens):\n",
    "    for i, token in enumerate(tokens):\n",
    "\n",
    "        # ignore terminals\n",
    "        if is_terminal(token): continue\n",
    "\n",
    "        # choose a replacement at random\n",
    "        replacement = random.choice(grammar[token])\n",
    "\n",
    "        if is_terminal(replacement):\n",
    "            tokens[i] = replacement\n",
    "        else:\n",
    "            tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n",
    "        return expand(grammar, tokens)\n",
    "\n",
    "    # if we get here we had all terminals and are done\n",
    "    return tokens\n",
    "\n",
    "def generate_sentence(grammar):\n",
    "    return expand(grammar, [\"_S\"])\n",
    "\n",
    "#\n",
    "# Gibbs Sampling\n",
    "#\n",
    "\n",
    "def roll_a_die():\n",
    "    return random.choice([1,2,3,4,5,6])\n",
    "\n",
    "def direct_sample():\n",
    "    d1 = roll_a_die()\n",
    "    d2 = roll_a_die()\n",
    "    return d1, d1 + d2\n",
    "\n",
    "def random_y_given_x(x):\n",
    "    \"\"\"equally likely to be x + 1, x + 2, ... , x + 6\"\"\"\n",
    "    return x + roll_a_die()\n",
    "\n",
    "def random_x_given_y(y):\n",
    "    if y <= 7:\n",
    "        # if the total is 7 or less, the first die is equally likely to be\n",
    "        # 1, 2, ..., (total - 1)\n",
    "        return random.randrange(1, y)\n",
    "    else:\n",
    "        # if the total is 7 or more, the first die is equally likely to be\n",
    "        # (total - 6), (total - 5), ..., 6\n",
    "        return random.randrange(y - 6, 7)\n",
    "\n",
    "def gibbs_sample(num_iters=100):\n",
    "    x, y = 1, 2 # doesn't really matter\n",
    "    for _ in range(num_iters):\n",
    "        x = random_x_given_y(y)\n",
    "        y = random_y_given_x(x)\n",
    "    return x, y\n",
    "\n",
    "def compare_distributions(num_samples=1000):\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for _ in range(num_samples):\n",
    "        counts[gibbs_sample()][0] += 1\n",
    "        counts[direct_sample()][1] += 1\n",
    "    return counts\n",
    "\n",
    "#\n",
    "# TOPIC MODELING\n",
    "#\n",
    "\n",
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()       # uniform between 0 and total\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w                        # return the smallest i such that\n",
    "        if rnd <= 0: return i           # sum(weights[:(i+1)]) >= rnd\n",
    "\n",
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]\n",
    "\n",
    "K = 4\n",
    "\n",
    "document_topic_counts = [Counter()\n",
    "                         for _ in documents]\n",
    "\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "document_lengths = map(len, documents)\n",
    "\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "D = len(documents)\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    \"\"\"the fraction of words in document _d_\n",
    "    that are assigned to _topic_ (plus some smoothing)\"\"\"\n",
    "\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    \"\"\"the fraction of words assigned to _topic_\n",
    "    that equal _word_ (plus some smoothing)\"\"\"\n",
    "\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    \"\"\"given a document and a word in that document,\n",
    "    return the weight for the k-th topic\"\"\"\n",
    "\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "\n",
    "            # remove this word / topic from the counts\n",
    "            # so that it doesn't influence the weights\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # and now add it back to the counts\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram sentences\n",
      "0 The result was probably generated by making data is highly regarded books members had built data scientist at profiles LinkedIn's membership .\n",
      "1 ly is divided into a few years ago in their own story isn't really what they are associated with data is missing or communicate the calculation it that's not know or getting humans or some hints at what kind of successful businesses will be widely applicable to work .\n",
      "2 Mobile applications to put to think outside the heart of a recommendation engine is telling .\n",
      "3 It was insufficient .\n",
      "4 These are associated with patience the track sends it .\n",
      "5 The web and put to Cornell Then at roughly 25 GB a training data itself but all heard big data into a lot about statistics work .\n",
      "6 And it's about how to be correlated with the numbers .\n",
      "7 The most of data analysis group together at events that data scientist at the state of ozone layer depletion was presented as awk to figure out whether this data that's not be ready for exploring and network administration .\n",
      "8 That's an excellent way to go beyond a data contributed by neighborhood per gram bits per gram .\n",
      "9 If data analysis .\n",
      "\n",
      "trigram sentences\n",
      "0 Do you really care if you want to know which originated with Apple you can follow it up with new ways to view the problem isn't finding data it's a data scientist .\n",
      "1 Most of the organizations that have built data platforms have found a home at Cloudera which provides commercial support .\n",
      "2 ly are following their path .\n",
      "3 To understand what correlation means .\n",
      "4 According to Jeff Hammerbacher in Beautiful Data Mike Loukides is Vice President of Content Strategy for O'Reilly Media Inc .\n",
      "5 Their business is fundamentally different from selling music sharing music or analyzing musical tastes though these can also be data products .\n",
      "6 Facebook and LinkedIn use patterns of friendship relationships to suggest other people you may not know what's important until after you've analyzed the data analysis group at a cost of a data platform though .\n",
      "7 . .\n",
      "8 What are we trying to build interesting products from it The future belongs to the companies and the tools for working with the number of other databases and data services credit card processing companies banks and so on .\n",
      "9 What's less obvious is that they generate a trail of data and you have to look for creative visualizations .\n",
      "\n",
      "grammar sentences\n",
      "0 big regression near big Python learns big regression about big Python\n",
      "1 logistic big linear linear Python about big Python near big Python about linear regression about logistic Python tests\n",
      "2 regression is regression\n",
      "3 logistic logistic logistic regression about logistic Python near big data science near linear regression tests big regression near logistic regression\n",
      "4 data science tests\n",
      "5 linear regression near big Python learns logistic big Python about linear data science near logistic data science\n",
      "6 data science is data science\n",
      "7 Python learns Python\n",
      "8 regression trains\n",
      "9 logistic big big big logistic big regression about linear data science about big data science about linear Python near big Python near big Python about logistic data science trains\n",
      "\n",
      "gibbs sampling\n",
      "(5, 9) 37 21\n",
      "(6, 9) 27 26\n",
      "(1, 3) 31 21\n",
      "(4, 8) 33 26\n",
      "(5, 6) 33 33\n",
      "(2, 8) 38 29\n",
      "(4, 7) 23 34\n",
      "(1, 6) 29 36\n",
      "(3, 7) 25 31\n",
      "(2, 5) 31 22\n",
      "(5, 8) 30 20\n",
      "(1, 2) 25 27\n",
      "(6, 7) 20 27\n",
      "(6, 10) 27 23\n",
      "(1, 5) 26 31\n",
      "(3, 6) 30 24\n",
      "(4, 9) 23 30\n",
      "(4, 10) 21 27\n",
      "(2, 6) 36 19\n",
      "(5, 11) 28 38\n",
      "(4, 5) 28 29\n",
      "(6, 11) 29 30\n",
      "(1, 4) 23 35\n",
      "(3, 9) 26 20\n",
      "(2, 3) 26 31\n",
      "(6, 12) 15 32\n",
      "(6, 8) 21 30\n",
      "(2, 7) 23 32\n",
      "(5, 10) 25 31\n",
      "(4, 6) 36 25\n",
      "(5, 7) 34 29\n",
      "(3, 8) 36 25\n",
      "(3, 5) 23 28\n",
      "(1, 7) 32 27\n",
      "(3, 4) 30 25\n",
      "(2, 4) 20 26\n",
      "0 Java 3\n",
      "0 Big Data 3\n",
      "0 Hadoop 2\n",
      "0 deep learning 2\n",
      "0 artificial intelligence 2\n",
      "0 C++ 2\n",
      "0 neural networks 1\n",
      "0 Storm 1\n",
      "0 programming languages 1\n",
      "0 MapReduce 1\n",
      "0 Haskell 1\n",
      "1 R 4\n",
      "1 statistics 3\n",
      "1 Python 3\n",
      "1 probability 2\n",
      "1 pandas 2\n",
      "1 statsmodels 2\n",
      "1 mathematics 1\n",
      "1 numpy 1\n",
      "1 theory 1\n",
      "1 scipy 1\n",
      "2 HBase 3\n",
      "2 Postgres 2\n",
      "2 MongoDB 2\n",
      "2 Cassandra 2\n",
      "2 NoSQL 1\n",
      "2 MySQL 1\n",
      "2 Spark 1\n",
      "3 regression 3\n",
      "3 libsvm 2\n",
      "3 scikit-learn 2\n",
      "3 machine learning 2\n",
      "3 neural networks 1\n",
      "3 probability 1\n",
      "3 Mahout 1\n",
      "3 Python 1\n",
      "3 decision trees 1\n",
      "3 databases 1\n",
      "3 support vector machines 1\n",
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "Big Data and programming languages 4 databases 3\n",
      "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "databases 5\n",
      "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
      "Python and statistics 5 machine learning 1\n",
      "['R', 'Python', 'statistics', 'regression', 'probability']\n",
      "Python and statistics 4 machine learning 1\n",
      "['machine learning', 'regression', 'decision trees', 'libsvm']\n",
      "machine learning 4\n",
      "['Python', 'R', 'Java', 'C++', 'Haskell', 'programming languages']\n",
      "Big Data and programming languages 4 Python and statistics 1 machine learning 1\n",
      "['statistics', 'probability', 'mathematics', 'theory']\n",
      "Python and statistics 4\n",
      "['machine learning', 'scikit-learn', 'Mahout', 'neural networks']\n",
      "machine learning 4\n",
      "['neural networks', 'deep learning', 'Big Data', 'artificial intelligence']\n",
      "Big Data and programming languages 4\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "Big Data and programming languages 4\n",
      "['statistics', 'R', 'statsmodels']\n",
      "Python and statistics 3\n",
      "['C++', 'deep learning', 'artificial intelligence', 'probability']\n",
      "Big Data and programming languages 3 machine learning 1\n",
      "['pandas', 'R', 'Python']\n",
      "Python and statistics 3\n",
      "['databases', 'HBase', 'Postgres', 'MySQL', 'MongoDB']\n",
      "databases 4 machine learning 1\n",
      "['libsvm', 'regression', 'support vector machines']\n",
      "machine learning 3\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    document = get_document()\n",
    "\n",
    "    bigrams = zip(document, document[1:])\n",
    "    transitions = defaultdict(list)\n",
    "    for prev, current in bigrams:\n",
    "        transitions[prev].append(current)\n",
    "\n",
    "    random.seed(0)\n",
    "    print \"bigram sentences\"\n",
    "    for i in range(10):\n",
    "        print i, generate_using_bigrams(transitions)\n",
    "    print\n",
    "\n",
    "    # trigrams\n",
    "\n",
    "    trigrams = zip(document, document[1:], document[2:])\n",
    "    trigram_transitions = defaultdict(list)\n",
    "    starts = []\n",
    "\n",
    "    for prev, current, next in trigrams:\n",
    "\n",
    "        if prev == \".\":              # if the previous \"word\" was a period\n",
    "            starts.append(current)   # then this is a start word\n",
    "\n",
    "        trigram_transitions[(prev, current)].append(next)\n",
    "\n",
    "    print \"trigram sentences\"\n",
    "    for i in range(10):\n",
    "        print i, generate_using_trigrams(starts, trigram_transitions)\n",
    "    print\n",
    "\n",
    "    grammar = {\n",
    "        \"_S\"  : [\"_NP _VP\"],\n",
    "        \"_NP\" : [\"_N\",\n",
    "                 \"_A _NP _P _A _N\"],\n",
    "        \"_VP\" : [\"_V\",\n",
    "                 \"_V _NP\"],\n",
    "        \"_N\"  : [\"data science\", \"Python\", \"regression\"],\n",
    "        \"_A\"  : [\"big\", \"linear\", \"logistic\"],\n",
    "        \"_P\"  : [\"about\", \"near\"],\n",
    "        \"_V\"  : [\"learns\", \"trains\", \"tests\", \"is\"]\n",
    "    }\n",
    "\n",
    "    print \"grammar sentences\"\n",
    "    for i in range(10):\n",
    "        print i, \" \".join(generate_sentence(grammar))\n",
    "    print\n",
    "\n",
    "    print \"gibbs sampling\"\n",
    "    comparison = compare_distributions()\n",
    "    for roll, (gibbs, direct) in comparison.iteritems():\n",
    "        print roll, gibbs, direct\n",
    "\n",
    "\n",
    "    # topic MODELING\n",
    "\n",
    "    for k, word_counts in enumerate(topic_word_counts):\n",
    "        for word, count in word_counts.most_common():\n",
    "            if count > 0: print k, word, count\n",
    "\n",
    "    topic_names = [\"Big Data and programming languages\",\n",
    "                   \"Python and statistics\",\n",
    "                   \"databases\",\n",
    "                   \"machine learning\"]\n",
    "\n",
    "    for document, topic_counts in zip(documents, document_topic_counts):\n",
    "        print document\n",
    "        for topic, count in topic_counts.most_common():\n",
    "            if count > 0:\n",
    "                print topic_names[topic], count,\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
